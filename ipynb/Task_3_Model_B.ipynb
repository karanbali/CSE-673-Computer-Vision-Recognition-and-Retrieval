{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Task_3_Model_B.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EA7VaH7Ydxy"
      },
      "source": [
        "# Task 3: Model_B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm3JXBTYMGnx"
      },
      "source": [
        "### Import Dependencies and download \"Tiny-ImageNet-200\" dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBnXqvWAST1U",
        "outputId": "9d07796e-17c3-4c2c-d609-032a03495ebb"
      },
      "source": [
        "# Mounting Google drive for loading the checkpoint.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cd gdrive/MyDrive/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeloaBMkxvhi",
        "outputId": "28beee40-55b6-44ef-eff2-96db5d0fae54"
      },
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip && ls tiny-imagenet-200\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "import torchvision.models as models\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "cuda = torch.device('cuda') \n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-05 22:52:01--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  12.5MB/s    in 19s     \n",
            "\n",
            "2021-11-05 22:52:20 (12.7 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n",
            "test  train  val  wnids.txt  words.txt\n",
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzDGwZoxMUHX"
      },
      "source": [
        "### Preprocessing: Collate all image's path address & labels as a input to custom pytorch dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HVW1-sJyWsJ"
      },
      "source": [
        "file_list = []\n",
        "\n",
        "\n",
        "for folder in os.listdir('./tiny-imagenet-200/train/'):\n",
        "\n",
        "\n",
        "  label = folder \n",
        "  for file in os.listdir('./tiny-imagenet-200/train/' + folder + '/images/'):\n",
        "    file_dir = './tiny-imagenet-200/train/' + folder + '/images/' + file\n",
        "\n",
        "    file_list.append((file_dir))\n",
        "\n",
        "with open('./tiny-imagenet-200/wnids.txt',) as f:\n",
        "\n",
        "  id_list = {}\n",
        "  read_data = f.readlines()\n",
        "  for i, val in enumerate(read_data):\n",
        "    id_list[val.replace('\\n', '')] = i\n",
        "\n",
        "\n",
        "test_list = []\n",
        "test_id = {}\n",
        "with open('./tiny-imagenet-200/val/val_annotations.txt', 'r') as f:\n",
        "  for line in f.readlines():\n",
        "    file, label = line.split()[0:2]\n",
        "    file_dir = './tiny-imagenet-200/val/images/' + file\n",
        "\n",
        "    test_list.append((file_dir))\n",
        "    test_id[file] = label"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQwYYGpPNdu-"
      },
      "source": [
        "### Custom Pytorch datatset for training images of \"Tiny-ImageNet-200\" dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EytwpRzykLF"
      },
      "source": [
        "class TrainTinyImageNetDataset(Dataset):\n",
        "    def __init__(self, f_list, id, transform=None):\n",
        "\n",
        "        self.filenames = f_list\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_path = self.filenames[idx]\n",
        "        image = None\n",
        "       \n",
        "        with open(img_path, 'rb') as f:\n",
        "          image = Image.open(f)\n",
        "          image =  image.convert('RGB')\n",
        "          \n",
        "        \n",
        "       \n",
        "\n",
        "        label = self.id_dict[img_path.split('/')[-1].split('.')[0].split('_')[0]]\n",
        "       \n",
        "\n",
        "        if self.transform is not None:\n",
        "\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label\n",
        "\n",
        "\n",
        "class TrainSet(TrainTinyImageNetDataset):\n",
        "  \n",
        "    def __init__(self, f_list, id, transform=None):\n",
        "\n",
        "        super(TrainSet, self).__init__(f_list, id, transform=transform)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2VBY3rZNpiS"
      },
      "source": [
        "### Custom Pytorch datatset for testing images of \"Tiny-ImageNet-200\" dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KOWAGrxy2-E"
      },
      "source": [
        "class TestTinyImageNetDataset(Dataset):\n",
        "    def __init__(self, t_list, id, cls_id, transform=None):\n",
        "        self.filenames = t_list\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "        self.cls_id = cls_id\n",
        "       \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.filenames[idx]\n",
        "        image = None\n",
        "       \n",
        "        with open(img_path, 'rb') as f:\n",
        "          image = Image.open(f)\n",
        "          image =  image.convert('RGB')\n",
        "    \n",
        "        label = self.cls_id[self.id_dict[img_path.split('/')[-1]]]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "       \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFlNKw85ON_7"
      },
      "source": [
        "### CUBS Block 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5arFr_LzNJd"
      },
      "source": [
        "class CUBS1(nn.Module):\n",
        "  def __init__(self, channels_in, n_in):\n",
        "      super(CUBS1, self).__init__()\n",
        "      self.channels_in = channels_in\n",
        "      self.n_in = n_in\n",
        "      self.dense_1_1 = nn.Linear(channels_in,n_in)\n",
        "      self.dense_1_2 = nn.Linear(channels_in,n_in)\n",
        "      self.dense_1_3 = nn.Linear(channels_in,n_in)\n",
        "      self.dense_2 = nn.Linear(n_in,channels_in)\n",
        "      self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    \n",
        "\n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "      self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def CB1_Sim1(self, a, b):\n",
        "    a_norm = torch.nn.functional.normalize(a, dim=2)\n",
        "    b_norm = torch.nn.functional.normalize(b, dim=2)\n",
        "\n",
        "    sim_matrix = torch.matmul(a_norm[0].T, b_norm[0])\n",
        "    sim_matrix = sim_matrix.reshape((1,self.n_in,self.n_in))\n",
        "\n",
        "\n",
        "    for i in range(1,a.shape[0]):\n",
        "  \n",
        "      mul = torch.matmul(a_norm[i].T, b_norm[i]) \n",
        "      mul = mul.reshape((1,self.n_in,self.n_in))\n",
        "      sim_matrix = torch.vstack((sim_matrix,mul))\n",
        "      \n",
        "\n",
        "    \n",
        "\n",
        "    return sim_matrix\n",
        "\n",
        "\n",
        "  def CB1_Sim2(self, a, b):\n",
        "    b_norm = torch.nn.functional.normalize(b, dim=2)\n",
        "\n",
        "    sim_matrix = torch.matmul(a[0], b_norm[0].T)\n",
        "    sim_matrix = sim_matrix.reshape((1,1,self.n_in))\n",
        "\n",
        "\n",
        "    for i in range(1,a.shape[0]):\n",
        "      mul = torch.matmul(a[i], b_norm[i].T)\n",
        "      mul = mul.reshape((1,1,self.n_in))\n",
        "      sim_matrix = torch.vstack((sim_matrix,mul))\n",
        "\n",
        "    return sim_matrix\n",
        "\n",
        "  def CB1_Softmax(self, a):\n",
        "    out_matrix = nn.Softmax(dim=1)(a[0])\n",
        "    out_matrix = out_matrix.reshape((1,self.n_in,self.n_in))\n",
        "\n",
        "    for i in range(1,a.shape[0]):\n",
        "      sim = nn.Softmax(dim=1)(a[i])\n",
        "      sim = sim.reshape((1,self.n_in,self.n_in))\n",
        "      out_matrix = torch.vstack((out_matrix,sim))\n",
        "\n",
        "    return out_matrix\n",
        "\n",
        "  def CB1_Channel(self, a, b):\n",
        "    out_matrix = b[0] * a[0].unsqueeze(dim=-1). unsqueeze(dim=-1)\n",
        "\n",
        "    for i in range(1,a.shape[0]): \n",
        "      mul = b[i] * a[i].unsqueeze(dim=-1). unsqueeze(dim=-1)\n",
        "      out_matrix = torch.vstack((out_matrix,mul))\n",
        "\n",
        "    return out_matrix\n",
        "\n",
        "      \n",
        "  \n",
        "\n",
        "  def forward(self, x):\n",
        "      x_pool = self.avgpool(x)\n",
        "      x_pool = torch.reshape(x_pool,(x.shape[0],1,x.shape[1]))\n",
        "\n",
        "     \n",
        "\n",
        "      x_1_1 = self.dense_1_1(x_pool)\n",
        "     \n",
        "      \n",
        "      x_1_1 = self.relu(x_1_1)\n",
        "\n",
        "      x_1_2 = self.dense_1_2(x_pool)\n",
        "      x_1_2 = self.relu(x_1_2)\n",
        "\n",
        "      x_1_3 = self.dense_1_3(x_pool)\n",
        "      x_1_3 = self.relu(x_1_3)\n",
        "\n",
        "     \n",
        "      x_1_2_sim = self.CB1_Sim1(x_1_1, x_1_2)\n",
        "\n",
        "\n",
        "            \n",
        "      x_1_2_softmax = self.CB1_Softmax(x_1_2_sim)\n",
        "\n",
        "  \n",
        "      x_1_2_3_sim = self.CB1_Sim2(x_1_3, x_1_2_softmax)\n",
        "      \n",
        "      x_2 = self.dense_2(x_1_2_3_sim)\n",
        "      x_2 = self.relu(x_2)\n",
        "\n",
        "      x_concat_1 = torch.add(x_2, x_pool)\n",
        "      x_sig = self.sig(x_concat_1)\n",
        "\n",
        "\n",
        "      x_out = self.CB1_Channel(x_sig, x)\n",
        "      return x_out\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYc-zspuOS89"
      },
      "source": [
        "### CUBS Block 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EJGDFNEzXda"
      },
      "source": [
        "\n",
        "class CUBS2(nn.Module):\n",
        "  def __init__(self, channels_in, n_in):\n",
        "      super(CUBS2, self).__init__()\n",
        "      self.channels_in = channels_in\n",
        "      self.n_in = n_in\n",
        "\n",
        "\n",
        "      self.conv_1_1 = nn.Conv2d(channels_in, 1, kernel_size=1)\n",
        "      self.conv_1_2 = nn.Conv2d(channels_in, 1, kernel_size=1)\n",
        "      self.conv_1_3 = nn.Conv2d(channels_in, 1, kernel_size=1)\n",
        "      \n",
        "   \n",
        "      \n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "      self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def CB2_Sim1(self, a, b):\n",
        "\n",
        "      a = torch.flatten(a, start_dim=2, end_dim=3)    \n",
        "      b = torch.flatten(b, start_dim=2, end_dim=3)\n",
        "\n",
        "      a_norm = torch.nn.functional.normalize(a, dim=2)\n",
        "      b_norm = torch.nn.functional.normalize(b, dim=2)\n",
        "\n",
        "      sim_matrix = torch.matmul(a_norm[0].T, b_norm[0])\n",
        "      sim_matrix = sim_matrix.reshape((1,a.shape[2],a.shape[2]))\n",
        "\n",
        "\n",
        "      for i in range(1,a.shape[0]):\n",
        "    \n",
        "        mul = torch.matmul(a_norm[i].T, b_norm[i]) \n",
        "        mul = mul.reshape((1,a.shape[2],a.shape[2]))\n",
        "        sim_matrix = torch.vstack((sim_matrix,mul))\n",
        "        \n",
        "\n",
        "      return sim_matrix\n",
        "\n",
        "\n",
        "  def CB2_Sim2(self, a, b):\n",
        "\n",
        "      a = torch.flatten(a, start_dim=2, end_dim=3)\n",
        "      b_norm = torch.nn.functional.normalize(b, dim=2)\n",
        "\n",
        "      sim_matrix = torch.matmul(a[0], b_norm[0].T)\n",
        "      sim_matrix = sim_matrix.reshape((1,1,a.shape[2]))\n",
        "\n",
        "\n",
        "      for i in range(1,a.shape[0]):\n",
        "        mul = torch.matmul(a[i], b_norm[i].T)\n",
        "        mul = mul.reshape((1,1,a.shape[2]))\n",
        "        sim_matrix = torch.vstack((sim_matrix,mul))\n",
        "\n",
        "      return sim_matrix\n",
        "\n",
        "\n",
        "  def CB2_Softmax(self, a):\n",
        "      out_matrix = nn.Softmax(dim=1)(a[0])\n",
        "      out_matrix = out_matrix.reshape((1,a.shape[2],a.shape[2]))\n",
        "\n",
        "      for i in range(1,a.shape[0]):\n",
        "        sim = nn.Softmax(dim=1)(a[i])\n",
        "        sim = sim.reshape((1,a.shape[2],a.shape[2]))\n",
        "        out_matrix = torch.vstack((out_matrix,sim))\n",
        "\n",
        "      return out_matrix\n",
        "\n",
        "  \n",
        "\n",
        "  def bmul(self, vec, mat, axis=0):\n",
        "      mat = mat.transpose(axis, -1)\n",
        "      return (mat * vec.expand_as(mat)).transpose(axis, -1)\n",
        "\n",
        "  def CB1_Pixel(self, a, b):\n",
        "      a = torch.reshape(a, (a.shape[0],b.shape[2],b.shape[3]))\n",
        "    \n",
        "      a_0 = a[0]\n",
        "      b_0 = b[0]\n",
        "      out_matrix = self.bmul(a_0,b_0, axis=2)\n",
        "      out_matrix = out_matrix.reshape((1,b.shape[1],b.shape[2],b.shape[3]))\n",
        "\n",
        "      for i in range(1, a.shape[0]):\n",
        "        ai = a[i]\n",
        "        \n",
        "        bi = b[i]\n",
        "      \n",
        "        mul = self.bmul(ai,bi, axis=2)\n",
        "        mul = mul.reshape((1,b.shape[1],b.shape[2],b.shape[3]))\n",
        "        out_matrix = torch.vstack((out_matrix,mul))\n",
        "\n",
        "      return out_matrix\n",
        "      \n",
        "  def forward(self, x):\n",
        "\n",
        "     \n",
        "      x_1_1 = self.conv_1_1(x)\n",
        "      x_1_1 = self.relu(x_1_1)\n",
        "\n",
        "      x_1_2 = self.conv_1_2(x)\n",
        "      x_1_2 = self.relu(x_1_2)\n",
        "\n",
        "      x_1_3 = self.conv_1_3(x)\n",
        "      x_1_3 = self.relu(x_1_3)\n",
        "\n",
        "     \n",
        "      x_1_2_sim = self.CB2_Sim1(x_1_1, x_1_2)\n",
        "\n",
        "      x_1_2_softmax = self.CB2_Softmax(x_1_2_sim)\n",
        "\n",
        "     \n",
        "      x_1_2_3_sim = self.CB2_Sim2(x_1_3, x_1_2_softmax)\n",
        "      \n",
        "      x_sig = self.sig(x_1_2_3_sim)\n",
        "\n",
        "     \n",
        "      x_out = self.CB1_Pixel(x_sig, x)\n",
        "      \n",
        "      return x_out\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6QmOSHhOYqj"
      },
      "source": [
        "### This section contains Open-Sourced \"ResNet-18\" implementation along with various modifications and CUBS Blocks(1 & 2) experiments mentioned in the assignment.\n",
        "\n",
        "***\n",
        "\n",
        "### I've modified the \"BasicBlock\" of open-sourced ResNet-18 implementations to get new \"BasicBlock_A\", \"BasicBlock_B\" and \"BasicBlock_C\" defining the all 3 different configurations that were given as a task.\n",
        "\n",
        "***\n",
        "\n",
        "### BasicBlock_A : Parallel CUBS 1 & CUBS 2\n",
        "### BasicBlock_B : CUBS 1 -> CUBS 2\n",
        "### BasicBlock_C : CUBS 2 -> CUBS 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaKzw2yczttS"
      },
      "source": [
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
        "           'wide_resnet50_2', 'wide_resnet101_2']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock_A(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock_A, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.cubs1 = CUBS1(planes, 30)\n",
        "        self.cubs2 = CUBS2(planes, 30)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # Parallel CUBS 1 & CUBS 2\n",
        "        out_cubs1 = self.cubs1(out)\n",
        "        out_cubs2 = self.cubs2(out)\n",
        "\n",
        "        out = out_cubs1 + out_cubs2\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class BasicBlock_B(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock_B, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.cubs1 = CUBS1(planes, 30)\n",
        "        self.cubs2 = CUBS2(planes, 30)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # CUBS 1 -> CUBS 2\n",
        "        out = self.cubs1(out)\n",
        "        out = self.cubs2(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class BasicBlock_C(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock_C, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.cubs1 = CUBS1(planes, 30)\n",
        "        self.cubs2 = CUBS2(planes, 30)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # CUBS 2 -> CUBS 1\n",
        "        out = self.cubs2(out)\n",
        "        out = self.cubs1(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=200, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWAyWPSrV7xg"
      },
      "source": [
        "### Defining 4 different models for vanilla ResNet-18, Model_A, Model_B & Model_C with different variations of \"BasicBlock\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLRRshHh0fOG"
      },
      "source": [
        "def resnet18(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
        "                   **kwargs)\n",
        "    \n",
        "\n",
        "def model_A(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock_A, [2, 2, 2, 2], pretrained, progress,\n",
        "                   **kwargs)\n",
        "    \n",
        "\n",
        "def model_B(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock_B, [2, 2, 2, 2], pretrained, progress,\n",
        "                   **kwargs)\n",
        "    \n",
        "\n",
        "def model_C(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock_C, [2, 2, 2, 2], pretrained, progress,\n",
        "                   **kwargs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t1FDzZyWSXF"
      },
      "source": [
        "### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGaWrdlCzzh2"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      \n",
        "        X, y = X.to(device), y.long().to(device)\n",
        "        optimizer.zero_grad()\n",
        "        torch.set_grad_enabled(True)\n",
        "        \n",
        "        pred = model(X).float()\n",
        "       \n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "       \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    \n",
        "        if batch % 1000 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv7QwhDXWXOV"
      },
      "source": [
        "### Testing Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pts-sMZ6z86K"
      },
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "          \n",
        "            X, y = X.to(device), y.long().to(device)\n",
        "            torch.set_grad_enabled(False)\n",
        "            pred = model(X).float()\n",
        "       \n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-YM-OZXWbvU"
      },
      "source": [
        "### Defining \"Learning Rate Scheduler\" & \"Early stopping mechanism\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irq8901N-Jo7"
      },
      "source": [
        "class LRScheduler():\n",
        "  \n",
        "    def __init__(\n",
        "        self, optimizer, patience=5, min_lr=1e-7, factor=0.75\n",
        "    ):\n",
        "        \n",
        "        self.optimizer = optimizer\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.factor = factor\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
        "                self.optimizer,\n",
        "                mode='min',\n",
        "                patience=self.patience,\n",
        "                factor=self.factor,\n",
        "                min_lr=self.min_lr,\n",
        "                verbose=True\n",
        "            )\n",
        "    def __call__(self, val_loss):\n",
        "        self.scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "class EarlyStopping():\n",
        "   \n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "      \n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss == None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        elif self.best_loss - val_loss < self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8l9hH17PiTc"
      },
      "source": [
        "### Instantiating Dataset loaders for training & testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4GR1wYJ-TUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff9b99b-3286-4712-c57e-4884b5261f4d"
      },
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "trans = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "trainset = TrainSet(f_list=file_list,id=id_list, transform=trans)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=5, shuffle=True, num_workers=4)\n",
        "\n",
        "trans_test = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "testset = TestTinyImageNetDataset(t_list=test_list,id=test_id, cls_id =id_list,  transform=trans_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False, num_workers=4)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAiLPad5XmnM"
      },
      "source": [
        "### Instantiating, training & checkpoint \"Model_B\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArGPyRN2Ps9C"
      },
      "source": [
        "#### There are 2 cells for \"Model_B\" that are given below.\n",
        "#### Run the 1st cell in case you want to load a checkpoint & resume training.\n",
        "#### Else Run the 2nd cell in case you want to start training from scratch.\n",
        "\n",
        "#### **NOTE: Make sure to change the path of loading the checkpoint in 1st cell**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RN0VK08P6wz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec11e739-fa03-49ad-db8c-b4f9ac8d7ed4"
      },
      "source": [
        "\n",
        "model_B = model_B(pretrained=False, progress=True)\n",
        "model_B = model_B.to(device)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer_model_B = torch.optim.Adam(model_B.parameters(), lr=0.0001)\n",
        "\n",
        "\n",
        "# **NOTE: Make sure to change the path of loading the checkpoint in 1st cell**\n",
        "checkpoint = torch.load('gdrive/MyDrive/Fall_21/trained-model_B-model.ckpt')\n",
        "\n",
        "\n",
        "model_B.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer_model_B.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch'] + 1\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "\n",
        "es =  EarlyStopping()\n",
        "lrs = LRScheduler(optimizer_model_B)\n",
        "\n",
        "checkpoint_dir = \".\"\n",
        "test_loss = 0\n",
        "epochs = 20\n",
        "for i in range(epochs):\n",
        "    print(f\"Epoch {i+1}\")\n",
        "    train(trainloader, model_B, loss_fn, optimizer_model_B)\n",
        "    test_loss = test(testloader, model_B, loss_fn)\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': i+1,\n",
        "        'model_state_dict': model_B.state_dict(),\n",
        "        'optimizer_state_dict': optimizer_model_B.state_dict(),\n",
        "        'loss': test_loss\n",
        "        }, checkpoint_dir+'/%04d-model_B-model.ckpt' %i)\n",
        "    \n",
        "    lrs(test_loss)\n",
        "    es(test_loss)\n",
        "    if es.early_stop:\n",
        "      break\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 4.859702  [    0/100000]\n",
            "loss: 3.742800  [ 5000/100000]\n",
            "loss: 4.344983  [10000/100000]\n",
            "loss: 4.853319  [15000/100000]\n",
            "loss: 4.573996  [20000/100000]\n",
            "loss: 3.761208  [25000/100000]\n",
            "loss: 3.307006  [30000/100000]\n",
            "loss: 1.851984  [35000/100000]\n",
            "loss: 2.662378  [40000/100000]\n",
            "loss: 1.783462  [45000/100000]\n",
            "loss: 3.498824  [50000/100000]\n",
            "loss: 1.307551  [55000/100000]\n",
            "loss: 3.138388  [60000/100000]\n",
            "loss: 2.859717  [65000/100000]\n",
            "loss: 3.120844  [70000/100000]\n",
            "loss: 2.826580  [75000/100000]\n",
            "loss: 3.052199  [80000/100000]\n",
            "loss: 2.966045  [85000/100000]\n",
            "loss: 5.392957  [90000/100000]\n",
            "loss: 3.507492  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 35.8%, Avg loss: 2.715523 \n",
            "\n",
            "Epoch 2\n",
            "loss: 1.823654  [    0/100000]\n",
            "loss: 2.597457  [ 5000/100000]\n",
            "loss: 1.756773  [10000/100000]\n",
            "loss: 4.262015  [15000/100000]\n",
            "loss: 3.025227  [20000/100000]\n",
            "loss: 1.973630  [25000/100000]\n",
            "loss: 2.249747  [30000/100000]\n",
            "loss: 2.565014  [35000/100000]\n",
            "loss: 2.726506  [40000/100000]\n",
            "loss: 2.526140  [45000/100000]\n",
            "loss: 3.554697  [50000/100000]\n",
            "loss: 4.018712  [55000/100000]\n",
            "loss: 2.218032  [60000/100000]\n",
            "loss: 2.225847  [65000/100000]\n",
            "loss: 1.679557  [70000/100000]\n",
            "loss: 2.408605  [75000/100000]\n",
            "loss: 3.788409  [80000/100000]\n",
            "loss: 2.408452  [85000/100000]\n",
            "loss: 4.694261  [90000/100000]\n",
            "loss: 3.771821  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 41.4%, Avg loss: 2.507010 \n",
            "\n",
            "Epoch 3\n",
            "loss: 1.573858  [    0/100000]\n",
            "loss: 1.833445  [ 5000/100000]\n",
            "loss: 4.162717  [10000/100000]\n",
            "loss: 1.563141  [15000/100000]\n",
            "loss: 2.031486  [20000/100000]\n",
            "loss: 1.747673  [25000/100000]\n",
            "loss: 2.272138  [30000/100000]\n",
            "loss: 1.454332  [35000/100000]\n",
            "loss: 2.822262  [40000/100000]\n",
            "loss: 1.930886  [45000/100000]\n",
            "loss: 1.642290  [50000/100000]\n",
            "loss: 2.813459  [55000/100000]\n",
            "loss: 1.521890  [60000/100000]\n",
            "loss: 2.398455  [65000/100000]\n",
            "loss: 2.887616  [70000/100000]\n",
            "loss: 3.114106  [75000/100000]\n",
            "loss: 1.486929  [80000/100000]\n",
            "loss: 2.282450  [85000/100000]\n",
            "loss: 2.444846  [90000/100000]\n",
            "loss: 1.670442  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 43.8%, Avg loss: 2.403873 \n",
            "\n",
            "Epoch 4\n",
            "loss: 2.733475  [    0/100000]\n",
            "loss: 1.410568  [ 5000/100000]\n",
            "loss: 1.463417  [10000/100000]\n",
            "loss: 3.003280  [15000/100000]\n",
            "loss: 1.498083  [20000/100000]\n",
            "loss: 2.369507  [25000/100000]\n",
            "loss: 1.539771  [30000/100000]\n",
            "loss: 2.144193  [35000/100000]\n",
            "loss: 1.310689  [40000/100000]\n",
            "loss: 2.127886  [45000/100000]\n",
            "loss: 1.737856  [50000/100000]\n",
            "loss: 2.814812  [55000/100000]\n",
            "loss: 1.738993  [60000/100000]\n",
            "loss: 2.628343  [65000/100000]\n",
            "loss: 0.611523  [70000/100000]\n",
            "loss: 1.258804  [75000/100000]\n",
            "loss: 1.864029  [80000/100000]\n",
            "loss: 3.249538  [85000/100000]\n",
            "loss: 3.039709  [90000/100000]\n",
            "loss: 1.068891  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 46.6%, Avg loss: 2.268810 \n",
            "\n",
            "Epoch 5\n",
            "loss: 1.716845  [    0/100000]\n",
            "loss: 2.815668  [ 5000/100000]\n",
            "loss: 2.399692  [10000/100000]\n",
            "loss: 1.136123  [15000/100000]\n",
            "loss: 1.733363  [20000/100000]\n",
            "loss: 1.912812  [25000/100000]\n",
            "loss: 5.057646  [30000/100000]\n",
            "loss: 0.997448  [35000/100000]\n",
            "loss: 1.121047  [40000/100000]\n",
            "loss: 2.130778  [45000/100000]\n",
            "loss: 3.698980  [50000/100000]\n",
            "loss: 1.800665  [55000/100000]\n",
            "loss: 0.540114  [60000/100000]\n",
            "loss: 2.663733  [65000/100000]\n",
            "loss: 0.914156  [70000/100000]\n",
            "loss: 1.688456  [75000/100000]\n",
            "loss: 1.612011  [80000/100000]\n",
            "loss: 2.168239  [85000/100000]\n",
            "loss: 1.013947  [90000/100000]\n",
            "loss: 2.762155  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 48.1%, Avg loss: 2.216512 \n",
            "\n",
            "Epoch 6\n",
            "loss: 1.416704  [    0/100000]\n",
            "loss: 2.565464  [ 5000/100000]\n",
            "loss: 2.456503  [10000/100000]\n",
            "loss: 1.222243  [15000/100000]\n",
            "loss: 2.325078  [20000/100000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f7dc8ae57d29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_model_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8edb1dd86951>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-624d36406c8f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-624d36406c8f>\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-624d36406c8f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# CUBS 1 -> CUBS 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcubs1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcubs2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-8cedfdc03880>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m       \u001b[0mx_1_2_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCB1_Sim1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_1_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-8cedfdc03880>\u001b[0m in \u001b[0;36mCB1_Sim1\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mCB1_Sim1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0ma_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mb_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[1;32m   4426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4427\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4428\u001b[0;31m         \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4429\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4430\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRgdP-M8Qe77"
      },
      "source": [
        "#### Else Run the 2nd cell in case you want to start training from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "woq-Q-Nt23uk",
        "outputId": "7662381e-36a2-40f3-a267-68ae3f43a153"
      },
      "source": [
        "model_B = model_B(pretrained=False, progress=True)\n",
        "model_B = model_B.to(device)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer_model_B = torch.optim.Adam(model_B.parameters(), lr=0.0001)\n",
        "\n",
        "es =  EarlyStopping()\n",
        "lrs = LRScheduler(optimizer_model_B)\n",
        "\n",
        "checkpoint_dir = \".\"\n",
        "test_loss = 0\n",
        "epochs = 20\n",
        "for i in range(epochs):\n",
        "    print(f\"Epoch {i+1}\")\n",
        "    train(trainloader, model_B, loss_fn, optimizer_model_B)\n",
        "    test_loss = test(testloader, model_B, loss_fn)\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': i+1,\n",
        "        'model_state_dict': model_B.state_dict(),\n",
        "        'optimizer_state_dict': optimizer_model_B.state_dict(),\n",
        "        'loss': test_loss\n",
        "        }, checkpoint_dir+'/%04d-model_B-model.ckpt' %i)\n",
        "    \n",
        "    lrs(test_loss)\n",
        "    es(test_loss)\n",
        "    if es.early_stop:\n",
        "      break\n",
        "\n",
        "\n",
        "torch.save({\n",
        "        'epoch': epochs,\n",
        "        'model_state_dict': model_B.state_dict(),\n",
        "        'optimizer_state_dict': optimizer_model_B.state_dict(),\n",
        "        'loss': test_loss\n",
        "        }, checkpoint_dir+'/final-model_B-model.ckpt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 5.422597  [    0/100000]\n",
            "loss: 5.091711  [ 5000/100000]\n",
            "loss: 5.999347  [10000/100000]\n",
            "loss: 4.826538  [15000/100000]\n",
            "loss: 4.147677  [20000/100000]\n",
            "loss: 4.448508  [25000/100000]\n",
            "loss: 5.043392  [30000/100000]\n",
            "loss: 4.540670  [35000/100000]\n",
            "loss: 4.936469  [40000/100000]\n",
            "loss: 5.702374  [45000/100000]\n",
            "loss: 5.114230  [50000/100000]\n",
            "loss: 3.911300  [55000/100000]\n",
            "loss: 4.452202  [60000/100000]\n",
            "loss: 4.408029  [65000/100000]\n",
            "loss: 3.963469  [70000/100000]\n",
            "loss: 5.045809  [75000/100000]\n",
            "loss: 4.727937  [80000/100000]\n",
            "loss: 4.878628  [85000/100000]\n",
            "loss: 3.073390  [90000/100000]\n",
            "loss: 3.789730  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 17.8%, Avg loss: 3.806427 \n",
            "\n",
            "Epoch 2\n",
            "loss: 3.465734  [    0/100000]\n",
            "loss: 3.922467  [ 5000/100000]\n",
            "loss: 4.040639  [10000/100000]\n",
            "loss: 3.516669  [15000/100000]\n",
            "loss: 5.024790  [20000/100000]\n",
            "loss: 4.830676  [25000/100000]\n",
            "loss: 4.024303  [30000/100000]\n",
            "loss: 4.108584  [35000/100000]\n",
            "loss: 5.327088  [40000/100000]\n",
            "loss: 2.612265  [45000/100000]\n",
            "loss: 3.749435  [50000/100000]\n",
            "loss: 3.616381  [55000/100000]\n",
            "loss: 4.672621  [60000/100000]\n",
            "loss: 3.673675  [65000/100000]\n",
            "loss: 3.641236  [70000/100000]\n",
            "loss: 2.564026  [75000/100000]\n",
            "loss: 4.386196  [80000/100000]\n",
            "loss: 3.900307  [85000/100000]\n",
            "loss: 3.881692  [90000/100000]\n",
            "loss: 4.272870  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 23.8%, Avg loss: 3.379165 \n",
            "\n",
            "Epoch 3\n",
            "loss: 3.725989  [    0/100000]\n",
            "loss: 4.116042  [ 5000/100000]\n",
            "loss: 2.550086  [10000/100000]\n",
            "loss: 3.368279  [15000/100000]\n",
            "loss: 2.736515  [20000/100000]\n",
            "loss: 2.794976  [25000/100000]\n",
            "loss: 3.122322  [30000/100000]\n",
            "loss: 4.051304  [35000/100000]\n",
            "loss: 2.604048  [40000/100000]\n",
            "loss: 3.760305  [45000/100000]\n",
            "loss: 2.971477  [50000/100000]\n",
            "loss: 2.980393  [60000/100000]\n",
            "loss: 4.501051  [65000/100000]\n",
            "loss: 3.809263  [70000/100000]\n",
            "loss: 2.510627  [75000/100000]\n",
            "loss: 1.769210  [80000/100000]\n",
            "loss: 4.392407  [85000/100000]\n",
            "loss: 2.808643  [90000/100000]\n",
            "loss: 3.836961  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 27.9%, Avg loss: 3.152324 \n",
            "\n",
            "Epoch 4\n",
            "loss: 3.665614  [    0/100000]\n",
            "loss: 2.873862  [ 5000/100000]\n",
            "loss: 3.010050  [10000/100000]\n",
            "loss: 3.256794  [15000/100000]\n",
            "loss: 2.808300  [20000/100000]\n",
            "loss: 2.195641  [25000/100000]\n",
            "loss: 1.449594  [30000/100000]\n",
            "loss: 4.067840  [35000/100000]\n",
            "loss: 3.112373  [40000/100000]\n",
            "loss: 1.584204  [45000/100000]\n",
            "loss: 2.798141  [50000/100000]\n",
            "loss: 4.245893  [55000/100000]\n",
            "loss: 3.865988  [60000/100000]\n",
            "loss: 4.775844  [65000/100000]\n",
            "loss: 3.078454  [70000/100000]\n",
            "loss: 2.932536  [75000/100000]\n",
            "loss: 2.764584  [80000/100000]\n",
            "loss: 3.181509  [85000/100000]\n",
            "loss: 3.793641  [90000/100000]\n",
            "loss: 3.645627  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 30.5%, Avg loss: 3.033274 \n",
            "\n",
            "Epoch 5\n",
            "loss: 3.182540  [    0/100000]\n",
            "loss: 3.388543  [ 5000/100000]\n",
            "loss: 3.211776  [10000/100000]\n",
            "loss: 2.507232  [15000/100000]\n",
            "loss: 3.577916  [20000/100000]\n",
            "loss: 2.482106  [25000/100000]\n",
            "loss: 1.725078  [30000/100000]\n",
            "loss: 2.963689  [35000/100000]\n",
            "loss: 1.465876  [40000/100000]\n",
            "loss: 3.519993  [45000/100000]\n",
            "loss: 4.627877  [50000/100000]\n",
            "loss: 3.515581  [55000/100000]\n",
            "loss: 3.087731  [60000/100000]\n",
            "loss: 1.433160  [65000/100000]\n",
            "loss: 2.454071  [70000/100000]\n",
            "loss: 2.050894  [75000/100000]\n",
            "loss: 3.640897  [80000/100000]\n",
            "loss: 2.111274  [85000/100000]\n",
            "loss: 1.908991  [90000/100000]\n",
            "loss: 3.207325  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 31.9%, Avg loss: 2.974386 \n",
            "\n",
            "Epoch 6\n",
            "loss: 3.270839  [    0/100000]\n",
            "loss: 4.361835  [ 5000/100000]\n",
            "loss: 3.417424  [10000/100000]\n",
            "loss: 1.646333  [15000/100000]\n",
            "loss: 2.490405  [20000/100000]\n",
            "loss: 1.736098  [25000/100000]\n",
            "loss: 2.635615  [30000/100000]\n",
            "loss: 2.659865  [35000/100000]\n",
            "loss: 3.829957  [40000/100000]\n",
            "loss: 4.156206  [45000/100000]\n",
            "loss: 1.988987  [50000/100000]\n",
            "loss: 3.276489  [55000/100000]\n",
            "loss: 3.628376  [60000/100000]\n",
            "loss: 3.096919  [65000/100000]\n",
            "loss: 2.102559  [70000/100000]\n",
            "loss: 2.357555  [75000/100000]\n",
            "loss: 2.997561  [80000/100000]\n",
            "loss: 2.505717  [85000/100000]\n",
            "loss: 3.129089  [90000/100000]\n",
            "loss: 1.514869  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 34.5%, Avg loss: 2.868586 \n",
            "\n",
            "Epoch 7\n",
            "loss: 2.993013  [    0/100000]\n",
            "loss: 2.717293  [ 5000/100000]\n",
            "loss: 2.292408  [10000/100000]\n",
            "loss: 2.615896  [15000/100000]\n",
            "loss: 3.010019  [20000/100000]\n",
            "loss: 2.490395  [25000/100000]\n",
            "loss: 2.257409  [30000/100000]\n",
            "loss: 1.047859  [35000/100000]\n",
            "loss: 3.110283  [40000/100000]\n",
            "loss: 3.297026  [45000/100000]\n",
            "loss: 3.717173  [50000/100000]\n",
            "loss: 3.377132  [55000/100000]\n",
            "loss: 2.440837  [60000/100000]\n",
            "loss: 2.532022  [65000/100000]\n",
            "loss: 3.708422  [70000/100000]\n",
            "loss: 2.047218  [75000/100000]\n",
            "loss: 2.315838  [80000/100000]\n",
            "loss: 2.999615  [85000/100000]\n",
            "loss: 2.442941  [90000/100000]\n",
            "loss: 1.930182  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 35.7%, Avg loss: 2.825573 \n",
            "\n",
            "Epoch 8\n",
            "loss: 2.794323  [    0/100000]\n",
            "loss: 2.090950  [ 5000/100000]\n",
            "loss: 2.450946  [10000/100000]\n",
            "loss: 2.235352  [15000/100000]\n",
            "loss: 1.298092  [20000/100000]\n",
            "loss: 3.555417  [25000/100000]\n",
            "loss: 3.305107  [30000/100000]\n",
            "loss: 3.932689  [35000/100000]\n",
            "loss: 2.355049  [40000/100000]\n",
            "loss: 3.768838  [45000/100000]\n",
            "loss: 2.985606  [50000/100000]\n",
            "loss: 2.578051  [55000/100000]\n",
            "loss: 1.959310  [60000/100000]\n",
            "loss: 1.531346  [65000/100000]\n",
            "loss: 3.077622  [70000/100000]\n",
            "loss: 2.871838  [75000/100000]\n",
            "loss: 1.769797  [80000/100000]\n",
            "loss: 1.164305  [85000/100000]\n",
            "loss: 3.236610  [90000/100000]\n",
            "loss: 2.991867  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 35.9%, Avg loss: 2.813601 \n",
            "\n",
            "Epoch 9\n",
            "loss: 3.757898  [    0/100000]\n",
            "loss: 2.996314  [ 5000/100000]\n",
            "loss: 1.413311  [10000/100000]\n",
            "loss: 1.452668  [15000/100000]\n",
            "loss: 2.867500  [20000/100000]\n",
            "loss: 3.738168  [25000/100000]\n",
            "loss: 1.335986  [30000/100000]\n",
            "loss: 2.076256  [35000/100000]\n",
            "loss: 2.087723  [40000/100000]\n",
            "loss: 3.499683  [45000/100000]\n",
            "loss: 2.465679  [50000/100000]\n",
            "loss: 2.705865  [55000/100000]\n",
            "loss: 3.293507  [60000/100000]\n",
            "loss: 1.691384  [65000/100000]\n",
            "loss: 1.385094  [70000/100000]\n",
            "loss: 1.579548  [75000/100000]\n",
            "loss: 2.162879  [80000/100000]\n",
            "loss: 1.841411  [85000/100000]\n",
            "loss: 2.419401  [90000/100000]\n",
            "loss: 2.678750  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 37.1%, Avg loss: 2.767387 \n",
            "\n",
            "Epoch 10\n",
            "loss: 3.250987  [    0/100000]\n",
            "loss: 2.284049  [ 5000/100000]\n",
            "loss: 1.664689  [10000/100000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-83e6e6a7c081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_model_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8edb1dd86951>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkhnwozxRjkb"
      },
      "source": [
        "### Instantiating, training & checkpoint \"Model_A\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1J02ab4m0BjI",
        "outputId": "d7eaf840-bf0e-4d8b-9b47-dd30a06961b6"
      },
      "source": [
        "model_A = model_A(pretrained=False, progress=True)\n",
        "model_A = model_A.to(device)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer_model_A = torch.optim.Adam(model_A.parameters(), lr=0.0001)\n",
        "\n",
        "es =  EarlyStopping()\n",
        "lrs = LRScheduler(optimizer_model_A)\n",
        "\n",
        "checkpoint_dir = \".\"\n",
        "test_loss = 0\n",
        "epochs = 20\n",
        "for i in range(epochs):\n",
        "    print(f\"Epoch {i+1}\")\n",
        "    train(trainloader, model_A, loss_fn, optimizer_model_A)\n",
        "    test_loss = test(testloader, model_A, loss_fn)\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': i+1,\n",
        "        'model_state_dict': model_A.state_dict(),\n",
        "        'optimizer_state_dict': optimizer_model_A.state_dict(),\n",
        "        'loss': test_loss\n",
        "        }, checkpoint_dir+'/%04d-model_A-model.ckpt' %i)\n",
        "    \n",
        "    lrs(test_loss)\n",
        "    es(test_loss)\n",
        "    if es.early_stop: \n",
        "         break\n",
        "\n",
        "\n",
        "torch.save({\n",
        "        'epoch': epochs,\n",
        "        'model_state_dict': model_A.state_dict(),\n",
        "        'optimizer_state_dict': optimizer_model_A.state_dict(),\n",
        "        'loss': test_loss\n",
        "        }, checkpoint_dir+'/final-model_A-model.ckpt')\n",
        "\n",
        "\n",
        "#optimizer = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 5.067235  [    0/100000]\n",
            "loss: 5.163366  [ 5000/100000]\n",
            "loss: 5.500050  [10000/100000]\n",
            "loss: 5.237983  [15000/100000]\n",
            "loss: 4.975750  [20000/100000]\n",
            "loss: 4.402113  [25000/100000]\n",
            "loss: 5.224821  [30000/100000]\n",
            "loss: 4.817933  [35000/100000]\n",
            "loss: 5.247917  [40000/100000]\n",
            "loss: 4.983361  [45000/100000]\n",
            "loss: 4.300044  [50000/100000]\n",
            "loss: 6.642183  [55000/100000]\n",
            "loss: 5.365790  [60000/100000]\n",
            "loss: 4.284121  [65000/100000]\n",
            "loss: 4.867014  [70000/100000]\n",
            "loss: 3.902377  [75000/100000]\n",
            "loss: 5.594351  [80000/100000]\n",
            "loss: 4.106315  [85000/100000]\n",
            "loss: 4.658834  [90000/100000]\n",
            "loss: 3.713499  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 13.9%, Avg loss: 4.002316 \n",
            "\n",
            "Epoch 2\n",
            "loss: 4.718019  [    0/100000]\n",
            "loss: 4.000587  [ 5000/100000]\n",
            "loss: 3.563755  [10000/100000]\n",
            "loss: 4.354568  [15000/100000]\n",
            "loss: 4.452868  [20000/100000]\n",
            "loss: 4.390163  [25000/100000]\n",
            "loss: 4.168198  [30000/100000]\n",
            "loss: 3.153057  [35000/100000]\n",
            "loss: 3.131476  [40000/100000]\n",
            "loss: 4.366388  [45000/100000]\n",
            "loss: 5.513882  [50000/100000]\n",
            "loss: 5.042588  [55000/100000]\n",
            "loss: 3.710127  [60000/100000]\n",
            "loss: 4.220350  [65000/100000]\n",
            "loss: 3.529835  [70000/100000]\n",
            "loss: 4.749357  [75000/100000]\n",
            "loss: 4.848959  [80000/100000]\n",
            "loss: 3.407541  [85000/100000]\n",
            "loss: 3.625489  [90000/100000]\n",
            "loss: 3.567232  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 20.9%, Avg loss: 3.546127 \n",
            "\n",
            "Epoch 3\n",
            "loss: 3.126713  [    0/100000]\n",
            "loss: 3.461339  [ 5000/100000]\n",
            "loss: 2.521122  [10000/100000]\n",
            "loss: 3.115187  [15000/100000]\n",
            "loss: 2.439051  [20000/100000]\n",
            "loss: 3.668372  [25000/100000]\n",
            "loss: 3.784908  [30000/100000]\n",
            "loss: 3.662882  [35000/100000]\n",
            "loss: 4.191910  [40000/100000]\n",
            "loss: 3.587159  [45000/100000]\n",
            "loss: 4.315634  [50000/100000]\n",
            "loss: 3.872337  [55000/100000]\n",
            "loss: 3.599678  [60000/100000]\n",
            "loss: 2.628078  [65000/100000]\n",
            "loss: 2.660314  [70000/100000]\n",
            "loss: 3.103904  [75000/100000]\n",
            "loss: 4.060752  [80000/100000]\n",
            "loss: 3.192542  [85000/100000]\n",
            "loss: 2.934620  [90000/100000]\n",
            "loss: 4.646834  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 26.2%, Avg loss: 3.254721 \n",
            "\n",
            "Epoch 4\n",
            "loss: 3.317866  [    0/100000]\n",
            "loss: 3.386312  [ 5000/100000]\n",
            "loss: 4.101024  [10000/100000]\n",
            "loss: 4.761172  [15000/100000]\n",
            "loss: 3.833650  [20000/100000]\n",
            "loss: 2.512338  [25000/100000]\n",
            "loss: 1.949215  [30000/100000]\n",
            "loss: 4.084925  [35000/100000]\n",
            "loss: 2.902421  [40000/100000]\n",
            "loss: 3.006405  [45000/100000]\n",
            "loss: 2.857580  [50000/100000]\n",
            "loss: 3.289387  [55000/100000]\n",
            "loss: 2.742346  [60000/100000]\n",
            "loss: 1.663316  [65000/100000]\n",
            "loss: 3.806571  [70000/100000]\n",
            "loss: 2.465774  [75000/100000]\n",
            "loss: 2.971374  [80000/100000]\n",
            "loss: 3.050884  [85000/100000]\n",
            "loss: 3.444833  [90000/100000]\n",
            "loss: 2.244044  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 28.7%, Avg loss: 3.108192 \n",
            "\n",
            "Epoch 5\n",
            "loss: 1.537760  [    0/100000]\n",
            "loss: 3.014910  [ 5000/100000]\n",
            "loss: 3.470271  [10000/100000]\n",
            "loss: 3.024035  [15000/100000]\n",
            "loss: 4.813993  [20000/100000]\n",
            "loss: 4.177541  [25000/100000]\n",
            "loss: 2.755505  [30000/100000]\n",
            "loss: 3.811085  [35000/100000]\n",
            "loss: 2.779349  [40000/100000]\n",
            "loss: 3.512130  [45000/100000]\n",
            "loss: 1.389392  [50000/100000]\n",
            "loss: 5.065948  [55000/100000]\n",
            "loss: 2.530224  [60000/100000]\n",
            "loss: 2.372695  [65000/100000]\n",
            "loss: 4.353616  [70000/100000]\n",
            "loss: 2.365803  [75000/100000]\n",
            "loss: 2.050691  [80000/100000]\n",
            "loss: 2.057137  [85000/100000]\n",
            "loss: 2.403127  [90000/100000]\n",
            "loss: 4.090520  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 31.2%, Avg loss: 3.028806 \n",
            "\n",
            "Epoch 6\n",
            "loss: 3.109343  [    0/100000]\n",
            "loss: 2.696247  [ 5000/100000]\n",
            "loss: 0.986466  [10000/100000]\n",
            "loss: 3.074236  [15000/100000]\n",
            "loss: 2.147756  [20000/100000]\n",
            "loss: 2.355216  [25000/100000]\n",
            "loss: 2.251706  [30000/100000]\n",
            "loss: 2.121356  [35000/100000]\n",
            "loss: 3.264627  [40000/100000]\n",
            "loss: 3.614639  [45000/100000]\n",
            "loss: 2.488116  [50000/100000]\n",
            "loss: 2.820026  [55000/100000]\n",
            "loss: 4.512563  [60000/100000]\n",
            "loss: 2.237295  [65000/100000]\n",
            "loss: 3.105102  [70000/100000]\n",
            "loss: 3.783918  [75000/100000]\n",
            "loss: 2.160353  [80000/100000]\n",
            "loss: 4.112039  [85000/100000]\n",
            "loss: 2.337288  [90000/100000]\n",
            "loss: 2.840219  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 33.3%, Avg loss: 2.880255 \n",
            "\n",
            "Epoch 7\n",
            "loss: 4.222163  [    0/100000]\n",
            "loss: 1.973211  [ 5000/100000]\n",
            "loss: 2.207902  [10000/100000]\n",
            "loss: 3.829044  [15000/100000]\n",
            "loss: 1.526047  [20000/100000]\n",
            "loss: 2.265676  [25000/100000]\n",
            "loss: 2.958382  [30000/100000]\n",
            "loss: 3.945803  [35000/100000]\n",
            "loss: 2.109027  [40000/100000]\n",
            "loss: 3.041589  [45000/100000]\n",
            "loss: 1.449599  [50000/100000]\n",
            "loss: 2.679960  [55000/100000]\n",
            "loss: 3.161037  [60000/100000]\n",
            "loss: 3.019826  [65000/100000]\n",
            "loss: 4.254552  [70000/100000]\n",
            "loss: 4.677915  [75000/100000]\n",
            "loss: 2.358694  [80000/100000]\n",
            "loss: 1.987797  [85000/100000]\n",
            "loss: 2.913646  [90000/100000]\n",
            "loss: 4.215878  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 35.2%, Avg loss: 2.806100 \n",
            "\n",
            "Epoch 8\n",
            "loss: 1.641321  [    0/100000]\n",
            "loss: 2.352355  [ 5000/100000]\n",
            "loss: 2.006080  [10000/100000]\n",
            "loss: 3.410360  [15000/100000]\n",
            "loss: 2.917880  [20000/100000]\n",
            "loss: 2.936918  [25000/100000]\n",
            "loss: 3.742191  [30000/100000]\n",
            "loss: 2.523602  [35000/100000]\n",
            "loss: 2.495305  [40000/100000]\n",
            "loss: 1.090389  [45000/100000]\n",
            "loss: 3.492684  [50000/100000]\n",
            "loss: 1.466770  [55000/100000]\n",
            "loss: 2.408741  [60000/100000]\n",
            "loss: 2.767925  [65000/100000]\n",
            "loss: 2.808357  [70000/100000]\n",
            "loss: 2.581273  [75000/100000]\n",
            "loss: 1.695217  [80000/100000]\n",
            "loss: 1.349126  [85000/100000]\n",
            "loss: 2.019827  [90000/100000]\n",
            "loss: 2.880446  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 36.6%, Avg loss: 2.765503 \n",
            "\n",
            "Epoch 9\n",
            "loss: 2.368870  [    0/100000]\n",
            "loss: 3.598380  [ 5000/100000]\n",
            "loss: 3.371230  [10000/100000]\n",
            "loss: 1.337592  [15000/100000]\n",
            "loss: 2.347047  [20000/100000]\n",
            "loss: 1.204489  [25000/100000]\n",
            "loss: 1.517130  [30000/100000]\n",
            "loss: 1.508850  [35000/100000]\n",
            "loss: 2.072689  [40000/100000]\n",
            "loss: 1.656625  [45000/100000]\n",
            "loss: 0.557457  [50000/100000]\n",
            "loss: 1.781517  [55000/100000]\n",
            "loss: 2.565530  [60000/100000]\n",
            "loss: 2.328295  [65000/100000]\n",
            "loss: 2.443520  [70000/100000]\n",
            "loss: 1.235064  [75000/100000]\n",
            "loss: 3.824125  [80000/100000]\n",
            "loss: 0.713507  [85000/100000]\n",
            "loss: 0.325611  [90000/100000]\n",
            "loss: 2.235512  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 36.2%, Avg loss: 2.755767 \n",
            "\n",
            "Epoch 10\n",
            "loss: 2.447232  [    0/100000]\n",
            "loss: 2.286419  [ 5000/100000]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d072b926df0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_model_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8edb1dd86951>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-624d36406c8f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-624d36406c8f>\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-624d36406c8f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Parallel CUBS 1 & CUBS 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mout_cubs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcubs1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mout_cubs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcubs2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_cubs1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_cubs2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e1bb94b106ca>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0mx_1_2_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCB2_Sim1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_1_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m       \u001b[0mx_1_2_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCB2_Softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1_2_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e1bb94b106ca>\u001b[0m in \u001b[0;36mCB2_Sim1\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmul\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0msim_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}