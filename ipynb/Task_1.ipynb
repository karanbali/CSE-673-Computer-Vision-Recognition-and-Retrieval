{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EA7VaH7Ydxy"
      },
      "source": [
        "# Task 1: My Model (Custom Model on Tiny-ImageNet-200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm3JXBTYMGnx"
      },
      "source": [
        "### Import Dependencies and download \"Tiny-ImageNet-200\" dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7Co0iJiwbjk",
        "outputId": "e24c8f86-257a-4203-ec1d-574d883e9d6d"
      },
      "source": [
        "# Mounting Google drive for loading the checkpoint.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cd gdrive/MyDrive/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeloaBMkxvhi",
        "outputId": "0eeb482f-eb1a-4aab-c764-0a38a916ba51"
      },
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip && ls tiny-imagenet-200\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "import torchvision.models as models\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "cuda = torch.device('cuda') \n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-05 19:45:26--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  31.4MB/s    in 10s     \n",
            "\n",
            "2021-11-05 19:45:36 (22.9 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n",
            "test  train  val  wnids.txt  words.txt\n",
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzDGwZoxMUHX"
      },
      "source": [
        "### Preprocessing: Collate all image's path address & labels as a input to custom pytorch dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HVW1-sJyWsJ"
      },
      "source": [
        "file_list = []\n",
        "\n",
        "\n",
        "for folder in os.listdir('./tiny-imagenet-200/train/'):\n",
        "\n",
        "\n",
        "  label = folder \n",
        "  for file in os.listdir('./tiny-imagenet-200/train/' + folder + '/images/'):\n",
        "    file_dir = './tiny-imagenet-200/train/' + folder + '/images/' + file\n",
        "\n",
        "    file_list.append((file_dir))\n",
        "\n",
        "with open('./tiny-imagenet-200/wnids.txt',) as f:\n",
        "\n",
        "  id_list = {}\n",
        "  read_data = f.readlines()\n",
        "  for i, val in enumerate(read_data):\n",
        "    id_list[val.replace('\\n', '')] = i\n",
        "\n",
        "\n",
        "test_list = []\n",
        "test_id = {}\n",
        "with open('./tiny-imagenet-200/val/val_annotations.txt', 'r') as f:\n",
        "  for line in f.readlines():\n",
        "    file, label = line.split()[0:2]\n",
        "    file_dir = './tiny-imagenet-200/val/images/' + file\n",
        "\n",
        "    test_list.append((file_dir))\n",
        "    test_id[file] = label"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQwYYGpPNdu-"
      },
      "source": [
        "### Custom Pytorch datatset for training images of \"Tiny-ImageNet-200\" dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EytwpRzykLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a5aafa-d5a9-4af6-8671-409388cacd18"
      },
      "source": [
        "class TrainTinyImageNetDataset(Dataset):\n",
        "    def __init__(self, f_list, id, transform=None):\n",
        "\n",
        "        self.filenames = f_list\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_path = self.filenames[idx]\n",
        "        image = None\n",
        "       \n",
        "        with open(img_path, 'rb') as f:\n",
        "          image = Image.open(f)\n",
        "          image =  image.convert('RGB')\n",
        "          \n",
        "        \n",
        "       \n",
        "\n",
        "        label = self.id_dict[img_path.split('/')[-1].split('.')[0].split('_')[0]]\n",
        "       \n",
        "\n",
        "        if self.transform is not None:\n",
        "\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label\n",
        "\n",
        "\n",
        "class TrainSet(TrainTinyImageNetDataset):\n",
        "  \n",
        "    def __init__(self, f_list, id, transform=None):\n",
        "\n",
        "        super(TrainSet, self).__init__(f_list, id, transform=transform)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2VBY3rZNpiS"
      },
      "source": [
        "### Custom Pytorch datatset for testing images of \"Tiny-ImageNet-200\" dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KOWAGrxy2-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20fad3d3-0e1c-4f59-c8f6-ba4502bc6695"
      },
      "source": [
        "class TestTinyImageNetDataset(Dataset):\n",
        "    def __init__(self, t_list, id, cls_id, transform=None):\n",
        "        self.filenames = t_list\n",
        "        self.transform = transform\n",
        "        self.id_dict = id\n",
        "        self.cls_id = cls_id\n",
        "       \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.filenames[idx]\n",
        "        image = None\n",
        "       \n",
        "        with open(img_path, 'rb') as f:\n",
        "          image = Image.open(f)\n",
        "          image =  image.convert('RGB')\n",
        "    \n",
        "        label = self.cls_id[self.id_dict[img_path.split('/')[-1]]]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "       "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlPq2nRKBIpL"
      },
      "source": [
        "### Defining \"MyModel\" (Custom Model for Task-1)\n",
        "\n",
        "### This model contain 7 Convolution Blocks (with Nomral Convolution, Batch Normalization & RELU Block Layers)\n",
        "\n",
        "### It also contains 4 Upsampling layers, 3 Skip-connections & a 2 layer classification head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIOnEHC94KFy"
      },
      "source": [
        "# My Model\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, stride=1, downsample=None):\n",
        "        super(MyModel, self).__init__()\n",
        "        \n",
        "        # Nomral Convolution, Batch Normalization & RELU Block Layers for 7 such blocks\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3)\n",
        "        self.bn7 = nn.BatchNorm2d(256)\n",
        "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3)\n",
        "        self.bn7 = nn.BatchNorm2d(512)\n",
        "\n",
        "\n",
        "        # Convolution layer for reducing the skip-connections to appropriate channel dimensions\n",
        "        self.out1_conv = nn.Conv2d(64, 128, kernel_size=1)\n",
        "        self.out2_conv = nn.Conv2d(128, 256, kernel_size=1)\n",
        "        self.out3_conv = nn.Conv2d(256, 512, kernel_size=1)\n",
        "\n",
        "\n",
        "        # Last FC & Classification layers\n",
        "        self.fc1 = nn.Linear(100352, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 200)\n",
        "\n",
        "        # Upsampling layers\n",
        "        self.up1 = nn.ConvTranspose2d(64, 64, 2, stride=2)\n",
        "        self.up2 = nn.ConvTranspose2d(128, 128, 2, stride=2)\n",
        "        self.up3 = nn.ConvTranspose2d(256, 256, 2, stride=2)\n",
        "        self.up4 = nn.ConvTranspose2d(512, 512, 2, stride=2)\n",
        "\n",
        "        # Pooling laye\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        \n",
        "  \n",
        "       \n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        # 1st CONV block\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.avgpool(out)\n",
        "        out1 = out\n",
        "        \n",
        "        # 1st Reducing Layer\n",
        "        out1 = self.out1_conv(out1)\n",
        "\n",
        "        # 1st Upsampling\n",
        "        out = self.up1(out)\n",
        "        \n",
        "        # 2nd CONV block\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.avgpool(out)\n",
        "\n",
        "        # 3rd CONV block\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.avgpool(out)\n",
        "\n",
        "        # 4th CONV block\n",
        "        out = self.conv4(out)\n",
        "        out = self.bn4(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.avgpool(out)\n",
        "\n",
        "        # 1st skip-connection \n",
        "        out = out + out1\n",
        "        out2 = out\n",
        "\n",
        "        # 2nd Reducing Layer\n",
        "        out2 = self.out2_conv(out2)\n",
        "\n",
        "        # 2nd Upsampling\n",
        "        out = self.up2(out)\n",
        "      \n",
        "        # 5th CONV block\n",
        "        out = self.conv5(out)\n",
        "        out = self.bn5(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.avgpool(out)\n",
        "\n",
        "        # # 6th CONV block\n",
        "        out = self.conv6(out)\n",
        "        out = self.bn6(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.avgpool(out)\n",
        "\n",
        "        # 2nd skip-connection\n",
        "        out = out + out2\n",
        "        out3 = out\n",
        "\n",
        "        # 3rd Reducing Layer\n",
        "        out3 = self.out3_conv(out3)\n",
        "\n",
        "\n",
        "        # 3rd Upsampling\n",
        "        out = self.up3(out)\n",
        "        \n",
        "        # 7th CONV block\n",
        "        out = self.conv7(out)\n",
        "        out = self.bn7(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.avgpool(out)\n",
        "\n",
        "        # 3rd skip-connection\n",
        "        out = out + out3\n",
        "\n",
        "        # 4th Upsampling\n",
        "        out = self.up4(out)\n",
        "        \n",
        "       \n",
        "        # Flatten\n",
        "        out = torch.flatten(out, 1)\n",
        "\n",
        "        # Last FC & Classification layers\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t1FDzZyWSXF"
      },
      "source": [
        "### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGaWrdlCzzh2"
      },
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      \n",
        "        X, y = X.to(device), y.long().to(device)\n",
        "        optimizer.zero_grad()\n",
        "        torch.set_grad_enabled(True)\n",
        "        \n",
        "        pred = model(X).float()\n",
        "       \n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "       \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    \n",
        "        if batch % 1000 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv7QwhDXWXOV"
      },
      "source": [
        "### Testing Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pts-sMZ6z86K"
      },
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "          \n",
        "            X, y = X.to(device), y.long().to(device)\n",
        "            torch.set_grad_enabled(False)\n",
        "            pred = model(X).float()\n",
        "       \n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-YM-OZXWbvU"
      },
      "source": [
        "### Defining \"Learning Rate Scheduler\" & \"Early stopping mechanism\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irq8901N-Jo7"
      },
      "source": [
        "class LRScheduler():\n",
        "  \n",
        "    def __init__(\n",
        "        self, optimizer, patience=5, min_lr=1e-7, factor=0.75\n",
        "    ):\n",
        "        \n",
        "        self.optimizer = optimizer\n",
        "        self.patience = patience\n",
        "        self.min_lr = min_lr\n",
        "        self.factor = factor\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
        "                self.optimizer,\n",
        "                mode='min',\n",
        "                patience=self.patience,\n",
        "                factor=self.factor,\n",
        "                min_lr=self.min_lr,\n",
        "                verbose=True\n",
        "            )\n",
        "    def __call__(self, val_loss):\n",
        "        self.scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "class EarlyStopping():\n",
        "   \n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "      \n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss == None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        elif self.best_loss - val_loss < self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IHXOVtbE-7o"
      },
      "source": [
        "### Instantiating Dataset loaders for training & testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4WSZEijlLHg",
        "outputId": "ebd61be6-21ea-4bc5-8b9e-ddef68102b67"
      },
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "trans = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "trainset = TrainSet(f_list=file_list,id=id_list, transform=trans)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=5, shuffle=True, num_workers=4)\n",
        "\n",
        "trans_test = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "testset = TestTinyImageNetDataset(t_list=test_list,id=test_id, cls_id =id_list,  transform=trans_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False, num_workers=4)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY1E2SucXi9U"
      },
      "source": [
        "### Instantiating & training & checkpoint \"MyModel\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_WNRoXNGH0j"
      },
      "source": [
        "#### There are 2 cells for \"MyModel\" that are given below.\n",
        "#### Run the 1st cell in case you want to load a checkpoint & resume training.\n",
        "#### Else Run the 2nd cell in case you want to start training from scratch.\n",
        "\n",
        "#### **NOTE: Make sure to change the path of loading the checkpoint in 1st cell**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kMICrzStlQcM",
        "outputId": "7bbac596-0af7-4476-b346-18ebf1802ee5"
      },
      "source": [
        "MyModel = MyModel()\n",
        "MyModel = MyModel.to(device)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer_MyModel = torch.optim.Adam(MyModel.parameters(), lr=0.0001)\n",
        "\n",
        "# **NOTE: Make sure to change the path of loading the checkpoint in 1st cell**\n",
        "checkpoint = torch.load('gdrive/MyDrive/trained-MyModel-model.ckpt')\n",
        "MyModel.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer_MyModel.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch'] + 1\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "es =  EarlyStopping()\n",
        "lrs = LRScheduler(optimizer_MyModel)\n",
        "\n",
        "\n",
        "checkpoint_dir = \".\"\n",
        "test_loss = 0\n",
        "epochs = 20\n",
        "\n",
        "for i in range(epochs):\n",
        "    print(f\"Epoch {i+1}\")\n",
        "    train(trainloader, MyModel, loss_fn, optimizer_MyModel)\n",
        "    test_loss = test(testloader, MyModel, loss_fn)\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': i+1,\n",
        "        'model_state_dict': MyModel.state_dict(),\n",
        "        'optimizer_state_dict': optimizer_MyModel.state_dict(),\n",
        "        'loss': test_loss\n",
        "        }, checkpoint_dir+'/%04d-MyModel-model.ckpt' %i)\n",
        "    \n",
        "    lrs(test_loss)\n",
        "    es(test_loss)\n",
        "    if es.early_stop: \n",
        "         break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 7.514592  [    0/100000]\n",
            "loss: 1.700148  [ 5000/100000]\n",
            "loss: 1.500712  [10000/100000]\n",
            "loss: 3.206714  [15000/100000]\n",
            "loss: 2.439969  [20000/100000]\n",
            "loss: 3.403428  [25000/100000]\n",
            "loss: 0.760292  [30000/100000]\n",
            "loss: 2.898070  [35000/100000]\n",
            "loss: 4.183686  [40000/100000]\n",
            "loss: 3.057768  [45000/100000]\n",
            "loss: 3.208254  [50000/100000]\n",
            "loss: 2.270092  [55000/100000]\n",
            "loss: 2.450874  [60000/100000]\n",
            "loss: 3.304353  [65000/100000]\n",
            "loss: 3.263307  [70000/100000]\n",
            "loss: 5.986336  [75000/100000]\n",
            "loss: 2.825410  [80000/100000]\n",
            "loss: 3.887670  [85000/100000]\n",
            "loss: 2.100561  [90000/100000]\n",
            "loss: 1.929519  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 26.6%, Avg loss: 3.347503 \n",
            "\n",
            "Epoch 2\n",
            "loss: 2.536825  [    0/100000]\n",
            "loss: 2.335321  [ 5000/100000]\n",
            "loss: 2.697928  [10000/100000]\n",
            "loss: 1.823586  [15000/100000]\n",
            "loss: 2.413153  [20000/100000]\n",
            "loss: 1.476184  [25000/100000]\n",
            "loss: 2.381083  [30000/100000]\n",
            "loss: 4.326852  [35000/100000]\n",
            "loss: 3.465562  [40000/100000]\n",
            "loss: 4.179102  [45000/100000]\n",
            "loss: 3.092170  [50000/100000]\n",
            "loss: 2.615019  [55000/100000]\n",
            "loss: 2.494379  [60000/100000]\n",
            "loss: 3.255557  [65000/100000]\n",
            "loss: 3.396289  [70000/100000]\n",
            "loss: 3.440315  [75000/100000]\n",
            "loss: 2.036773  [80000/100000]\n",
            "loss: 3.785967  [85000/100000]\n",
            "loss: 3.547130  [90000/100000]\n",
            "loss: 2.734053  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 26.9%, Avg loss: 3.323724 \n",
            "\n",
            "Epoch 3\n",
            "loss: 2.323485  [    0/100000]\n",
            "loss: 3.114217  [ 5000/100000]\n",
            "loss: 2.713037  [10000/100000]\n",
            "loss: 2.276113  [15000/100000]\n",
            "loss: 2.926997  [20000/100000]\n",
            "loss: 1.652403  [25000/100000]\n",
            "loss: 2.683420  [30000/100000]\n",
            "loss: 1.595748  [35000/100000]\n",
            "loss: 2.240187  [40000/100000]\n",
            "loss: 3.543658  [45000/100000]\n",
            "loss: 2.707929  [50000/100000]\n",
            "loss: 1.580778  [55000/100000]\n",
            "loss: 2.398080  [60000/100000]\n",
            "loss: 3.970824  [65000/100000]\n",
            "loss: 3.153782  [70000/100000]\n",
            "loss: 2.760123  [75000/100000]\n",
            "loss: 2.772063  [80000/100000]\n",
            "loss: 3.924086  [85000/100000]\n",
            "loss: 3.625036  [90000/100000]\n",
            "loss: 2.468602  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 27.7%, Avg loss: 3.321947 \n",
            "\n",
            "Epoch 4\n",
            "loss: 1.967536  [    0/100000]\n",
            "loss: 1.923683  [ 5000/100000]\n",
            "loss: 3.001652  [10000/100000]\n",
            "loss: 2.178890  [15000/100000]\n",
            "loss: 2.829328  [20000/100000]\n",
            "loss: 3.185512  [25000/100000]\n",
            "loss: 4.107162  [30000/100000]\n",
            "loss: 3.954317  [35000/100000]\n",
            "loss: 5.120628  [40000/100000]\n",
            "loss: 2.927902  [45000/100000]\n",
            "loss: 2.596141  [50000/100000]\n",
            "loss: 2.371111  [55000/100000]\n",
            "loss: 2.876842  [60000/100000]\n",
            "loss: 2.072152  [65000/100000]\n",
            "loss: 3.922321  [70000/100000]\n",
            "loss: 2.439939  [75000/100000]\n",
            "loss: 2.462308  [80000/100000]\n",
            "loss: 3.043156  [85000/100000]\n",
            "loss: 4.316866  [90000/100000]\n",
            "loss: 0.699984  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 26.7%, Avg loss: 3.349494 \n",
            "\n",
            "Epoch 5\n",
            "loss: 1.636732  [    0/100000]\n",
            "loss: 3.485069  [ 5000/100000]\n",
            "loss: 0.915795  [10000/100000]\n",
            "loss: 2.243743  [15000/100000]\n",
            "loss: 3.829898  [20000/100000]\n",
            "loss: 2.218422  [25000/100000]\n",
            "loss: 2.007060  [30000/100000]\n",
            "loss: 1.852097  [35000/100000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-89534e46213d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_MyModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-8edb1dd86951>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-4a556b31fa4c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sJxViAjGNqk"
      },
      "source": [
        "#### Run the 2nd cell in case you want to start training from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1J02ab4m0BjI",
        "outputId": "69c3508f-bc45-4d20-eef9-4f10ac93d34b"
      },
      "source": [
        "MyModel = MyModel()\n",
        "MyModel = MyModel.to(device)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer_MyModel = torch.optim.Adam(MyModel.parameters(), lr=0.0001)\n",
        "\n",
        "\n",
        "es =  EarlyStopping()\n",
        "lrs = LRScheduler(optimizer_MyModel)\n",
        "\n",
        "checkpoint_dir = \".\"\n",
        "test_loss = 0\n",
        "epochs = 20\n",
        "\n",
        "for i in range(epochs):\n",
        "    print(f\"Epoch {i+1}\")\n",
        "    train(trainloader, MyModel, loss_fn, optimizer_MyModel)\n",
        "    test_loss = test(testloader, MyModel, loss_fn)\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': i+1,\n",
        "        'model_state_dict': MyModel.state_dict(),\n",
        "        'optimizer_state_dict': optimizer_MyModel.state_dict(),\n",
        "        'loss': test_loss\n",
        "        }, checkpoint_dir+'/%04d-MyModel-model.ckpt' %i)\n",
        "    \n",
        "    lrs(test_loss)\n",
        "    es(test_loss)\n",
        "    if es.early_stop: \n",
        "         break\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 5.311528  [    0/100000]\n",
            "loss: 5.459359  [ 5000/100000]\n",
            "loss: 4.884769  [10000/100000]\n",
            "loss: 4.424979  [15000/100000]\n",
            "loss: 5.160327  [20000/100000]\n",
            "loss: 4.749619  [25000/100000]\n",
            "loss: 3.338456  [30000/100000]\n",
            "loss: 4.676524  [35000/100000]\n",
            "loss: 5.065419  [40000/100000]\n",
            "loss: 5.037518  [45000/100000]\n",
            "loss: 4.342889  [50000/100000]\n",
            "loss: 4.203567  [55000/100000]\n",
            "loss: 4.501772  [60000/100000]\n",
            "loss: 4.284214  [65000/100000]\n",
            "loss: 4.768407  [70000/100000]\n",
            "loss: 4.048433  [75000/100000]\n",
            "loss: 3.480863  [80000/100000]\n",
            "loss: 4.337773  [85000/100000]\n",
            "loss: 4.127688  [90000/100000]\n",
            "loss: 4.351015  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 12.6%, Avg loss: 4.222650 \n",
            "\n",
            "Epoch 2\n",
            "loss: 3.784712  [    0/100000]\n",
            "loss: 3.439715  [ 5000/100000]\n",
            "loss: 3.591293  [10000/100000]\n",
            "loss: 3.580187  [15000/100000]\n",
            "loss: 4.193014  [20000/100000]\n",
            "loss: 4.775201  [25000/100000]\n",
            "loss: 3.773143  [30000/100000]\n",
            "loss: 3.888266  [35000/100000]\n",
            "loss: 2.206846  [40000/100000]\n",
            "loss: 3.233969  [45000/100000]\n",
            "loss: 5.492607  [50000/100000]\n",
            "loss: 4.632821  [55000/100000]\n",
            "loss: 4.999615  [60000/100000]\n",
            "loss: 3.684158  [65000/100000]\n",
            "loss: 4.935279  [70000/100000]\n",
            "loss: 3.608599  [75000/100000]\n",
            "loss: 5.095483  [80000/100000]\n",
            "loss: 4.021200  [85000/100000]\n",
            "loss: 4.865016  [90000/100000]\n",
            "loss: 4.464365  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 16.5%, Avg loss: 3.921616 \n",
            "\n",
            "Epoch 3\n",
            "loss: 4.068855  [    0/100000]\n",
            "loss: 4.211331  [ 5000/100000]\n",
            "loss: 3.295938  [10000/100000]\n",
            "loss: 4.421887  [15000/100000]\n",
            "loss: 3.714739  [20000/100000]\n",
            "loss: 2.535910  [25000/100000]\n",
            "loss: 3.858727  [30000/100000]\n",
            "loss: 3.131918  [35000/100000]\n",
            "loss: 3.579743  [40000/100000]\n",
            "loss: 3.749372  [45000/100000]\n",
            "loss: 2.882431  [50000/100000]\n",
            "loss: 4.594537  [55000/100000]\n",
            "loss: 3.330910  [60000/100000]\n",
            "loss: 3.557132  [65000/100000]\n",
            "loss: 3.751684  [70000/100000]\n",
            "loss: 4.031404  [75000/100000]\n",
            "loss: 3.973420  [80000/100000]\n",
            "loss: 3.155744  [85000/100000]\n",
            "loss: 4.142649  [90000/100000]\n",
            "loss: 3.313941  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 20.5%, Avg loss: 3.651108 \n",
            "\n",
            "Epoch 4\n",
            "loss: 3.898005  [    0/100000]\n",
            "loss: 4.382628  [ 5000/100000]\n",
            "loss: 3.618613  [10000/100000]\n",
            "loss: 2.021986  [15000/100000]\n",
            "loss: 3.416029  [20000/100000]\n",
            "loss: 5.105091  [25000/100000]\n",
            "loss: 4.607438  [30000/100000]\n",
            "loss: 3.889177  [35000/100000]\n",
            "loss: 4.429735  [40000/100000]\n",
            "loss: 3.349725  [45000/100000]\n",
            "loss: 5.080326  [50000/100000]\n",
            "loss: 4.630918  [55000/100000]\n",
            "loss: 4.732388  [60000/100000]\n",
            "loss: 3.375033  [65000/100000]\n",
            "loss: 3.917053  [70000/100000]\n",
            "loss: 3.107362  [75000/100000]\n",
            "loss: 3.662021  [80000/100000]\n",
            "loss: 5.470054  [85000/100000]\n",
            "loss: 4.114301  [90000/100000]\n",
            "loss: 4.615586  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 23.3%, Avg loss: 3.515230 \n",
            "\n",
            "Epoch 5\n",
            "loss: 2.495889  [    0/100000]\n",
            "loss: 2.455194  [ 5000/100000]\n",
            "loss: 3.183433  [10000/100000]\n",
            "loss: 4.191039  [15000/100000]\n",
            "loss: 2.665077  [20000/100000]\n",
            "loss: 4.003386  [25000/100000]\n",
            "loss: 3.070925  [30000/100000]\n",
            "loss: 2.382639  [35000/100000]\n",
            "loss: 3.695316  [40000/100000]\n",
            "loss: 2.564615  [45000/100000]\n",
            "loss: 3.002976  [50000/100000]\n",
            "loss: 2.521127  [55000/100000]\n",
            "loss: 3.107163  [60000/100000]\n",
            "loss: 4.820905  [65000/100000]\n",
            "loss: 3.665138  [70000/100000]\n",
            "loss: 3.827922  [75000/100000]\n",
            "loss: 3.084235  [80000/100000]\n",
            "loss: 3.458953  [85000/100000]\n",
            "loss: 3.570523  [90000/100000]\n",
            "loss: 2.786387  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 24.8%, Avg loss: 3.391479 \n",
            "\n",
            "Epoch 6\n",
            "loss: 2.766233  [    0/100000]\n",
            "loss: 4.604083  [ 5000/100000]\n",
            "loss: 3.165224  [10000/100000]\n",
            "loss: 3.879391  [15000/100000]\n",
            "loss: 3.756135  [20000/100000]\n",
            "loss: 1.280380  [25000/100000]\n",
            "loss: 2.285102  [30000/100000]\n",
            "loss: 3.720914  [35000/100000]\n",
            "loss: 4.020129  [40000/100000]\n",
            "loss: 3.598436  [45000/100000]\n",
            "loss: 5.463562  [50000/100000]\n",
            "loss: 2.743203  [55000/100000]\n",
            "loss: 4.998803  [60000/100000]\n",
            "loss: 2.965379  [65000/100000]\n",
            "loss: 5.472398  [70000/100000]\n",
            "loss: 2.767058  [75000/100000]\n",
            "loss: 3.486815  [80000/100000]\n",
            "loss: 4.554340  [85000/100000]\n",
            "loss: 3.941481  [90000/100000]\n",
            "loss: 4.179668  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 26.1%, Avg loss: 3.399964 \n",
            "\n",
            "Epoch 7\n",
            "loss: 3.850920  [    0/100000]\n",
            "loss: 1.847461  [ 5000/100000]\n",
            "loss: 1.943748  [10000/100000]\n",
            "loss: 3.909502  [15000/100000]\n",
            "loss: 4.427348  [20000/100000]\n",
            "loss: 2.837787  [25000/100000]\n",
            "loss: 1.926114  [30000/100000]\n",
            "loss: 1.950467  [35000/100000]\n",
            "loss: 3.278318  [40000/100000]\n",
            "loss: 4.008806  [45000/100000]\n",
            "loss: 2.319962  [50000/100000]\n",
            "loss: 2.397154  [55000/100000]\n",
            "loss: 4.123257  [60000/100000]\n",
            "loss: 2.772814  [65000/100000]\n",
            "loss: 1.759989  [70000/100000]\n",
            "loss: 2.856678  [75000/100000]\n",
            "loss: 3.735585  [80000/100000]\n",
            "loss: 2.178051  [85000/100000]\n",
            "loss: 5.091650  [90000/100000]\n",
            "loss: 3.251355  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 27.0%, Avg loss: 3.329674 \n",
            "\n",
            "Epoch 8\n",
            "loss: 3.879731  [    0/100000]\n",
            "loss: 2.558691  [ 5000/100000]\n",
            "loss: 2.744041  [10000/100000]\n",
            "loss: 2.949052  [15000/100000]\n",
            "loss: 3.682861  [20000/100000]\n",
            "loss: 1.247232  [25000/100000]\n",
            "loss: 3.371458  [30000/100000]\n",
            "loss: 2.473195  [35000/100000]\n",
            "loss: 4.230167  [40000/100000]\n",
            "loss: 1.918903  [45000/100000]\n",
            "loss: 3.770993  [50000/100000]\n",
            "loss: 4.167804  [55000/100000]\n",
            "loss: 1.733095  [60000/100000]\n",
            "loss: 4.705871  [65000/100000]\n",
            "loss: 3.409711  [70000/100000]\n",
            "loss: 1.884184  [75000/100000]\n",
            "loss: 2.739618  [80000/100000]\n",
            "loss: 2.090946  [85000/100000]\n",
            "loss: 3.581538  [90000/100000]\n",
            "loss: 4.027690  [95000/100000]\n",
            "Test Error: \n",
            " Accuracy: 27.4%, Avg loss: 3.426925 \n",
            "\n",
            "Epoch 9\n",
            "loss: 3.117486  [    0/100000]\n",
            "loss: 2.065231  [ 5000/100000]\n",
            "loss: 3.525420  [10000/100000]\n",
            "loss: 3.518887  [15000/100000]\n",
            "loss: 2.112733  [20000/100000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fafece75e2bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_MyModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-8edb1dd86951>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}