# -*- coding: utf-8 -*-
"""Task_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YvEdauB8qhVoVoVYCtY86kKJ0fARngfg

# Task 1: My Model (Custom Model on Tiny-ImageNet-200)

### Import Dependencies and download "Tiny-ImageNet-200" dataset.
"""

# Mounting Google drive for loading the checkpoint.

from google.colab import drive
drive.mount('/content/gdrive')

!cd gdrive/MyDrive/

!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip
!unzip -q tiny-imagenet-200.zip && ls tiny-imagenet-200

import os
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision import transforms
from torchvision.transforms import ToTensor, Lambda
import torchvision.models as models
from torch.hub import load_state_dict_from_url


torch.cuda.empty_cache()
cuda = torch.device('cuda') 
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('Using {} device'.format(device))

"""### Preprocessing: Collate all image's path address & labels as a input to custom pytorch dataset"""

file_list = []


for folder in os.listdir('./tiny-imagenet-200/train/'):


  label = folder 
  for file in os.listdir('./tiny-imagenet-200/train/' + folder + '/images/'):
    file_dir = './tiny-imagenet-200/train/' + folder + '/images/' + file

    file_list.append((file_dir))

with open('./tiny-imagenet-200/wnids.txt',) as f:

  id_list = {}
  read_data = f.readlines()
  for i, val in enumerate(read_data):
    id_list[val.replace('\n', '')] = i


test_list = []
test_id = {}
with open('./tiny-imagenet-200/val/val_annotations.txt', 'r') as f:
  for line in f.readlines():
    file, label = line.split()[0:2]
    file_dir = './tiny-imagenet-200/val/images/' + file

    test_list.append((file_dir))
    test_id[file] = label

"""### Custom Pytorch datatset for training images of "Tiny-ImageNet-200" dataset."""

class TrainTinyImageNetDataset(Dataset):
    def __init__(self, f_list, id, transform=None):

        self.filenames = f_list
        self.transform = transform
        self.id_dict = id

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):

        img_path = self.filenames[idx]
        image = None
       
        with open(img_path, 'rb') as f:
          image = Image.open(f)
          image =  image.convert('RGB')
          
        
       

        label = self.id_dict[img_path.split('/')[-1].split('.')[0].split('_')[0]]
       

        if self.transform is not None:

            image = self.transform(image)
            
        return image, label


class TrainSet(TrainTinyImageNetDataset):
  
    def __init__(self, f_list, id, transform=None):

        super(TrainSet, self).__init__(f_list, id, transform=transform)

"""### Custom Pytorch datatset for testing images of "Tiny-ImageNet-200" dataset."""

class TestTinyImageNetDataset(Dataset):
    def __init__(self, t_list, id, cls_id, transform=None):
        self.filenames = t_list
        self.transform = transform
        self.id_dict = id
        self.cls_id = cls_id
       


    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        img_path = self.filenames[idx]
        image = None
       
        with open(img_path, 'rb') as f:
          image = Image.open(f)
          image =  image.convert('RGB')
    
        label = self.cls_id[self.id_dict[img_path.split('/')[-1]]]
        
        if self.transform is not None:
            image = self.transform(image)
        return image, label

"""### Defining "MyModel" (Custom Model for Task-1)

### This model contain 7 Convolution Blocks (with Nomral Convolution, Batch Normalization & RELU Block Layers)

### It also contains 4 Upsampling layers, 3 Skip-connections & a 2 layer classification head
"""

# My Model

class MyModel(nn.Module):
    expansion = 1

    def __init__(self, stride=1, downsample=None):
        super(MyModel, self).__init__()
        
        # Nomral Convolution, Batch Normalization & RELU Block Layers for 7 such blocks
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3)
        self.bn3 = nn.BatchNorm2d(64)
        self.conv4 = nn.Conv2d(64, 128, kernel_size=3)
        self.bn4 = nn.BatchNorm2d(128)
        self.conv5 = nn.Conv2d(128, 128, kernel_size=3)
        self.bn5 = nn.BatchNorm2d(128)
        self.conv6 = nn.Conv2d(128, 256, kernel_size=3)
        self.bn6 = nn.BatchNorm2d(256)
        self.conv7 = nn.Conv2d(256, 256, kernel_size=3)
        self.bn7 = nn.BatchNorm2d(256)
        self.conv7 = nn.Conv2d(256, 512, kernel_size=3)
        self.bn7 = nn.BatchNorm2d(512)


        # Convolution layer for reducing the skip-connections to appropriate channel dimensions
        self.out1_conv = nn.Conv2d(64, 128, kernel_size=1)
        self.out2_conv = nn.Conv2d(128, 256, kernel_size=1)
        self.out3_conv = nn.Conv2d(256, 512, kernel_size=1)


        # Last FC & Classification layers
        self.fc1 = nn.Linear(100352, 4096)
        self.fc2 = nn.Linear(4096, 200)

        # Upsampling layers
        self.up1 = nn.ConvTranspose2d(64, 64, 2, stride=2)
        self.up2 = nn.ConvTranspose2d(128, 128, 2, stride=2)
        self.up3 = nn.ConvTranspose2d(256, 256, 2, stride=2)
        self.up4 = nn.ConvTranspose2d(512, 512, 2, stride=2)

        # Pooling laye
        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))
        
  
       

    def forward(self, x):
        identity = x

        # 1st CONV block
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.avgpool(out)
        out1 = out
        
        # 1st Reducing Layer
        out1 = self.out1_conv(out1)

        # 1st Upsampling
        out = self.up1(out)
        
        # 2nd CONV block
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.avgpool(out)

        # 3rd CONV block
        out = self.conv3(out)
        out = self.bn3(out)
        out = self.relu(out)
        out = self.avgpool(out)

        # 4th CONV block
        out = self.conv4(out)
        out = self.bn4(out)
        out = self.relu(out)
        out = self.avgpool(out)

        # 1st skip-connection 
        out = out + out1
        out2 = out

        # 2nd Reducing Layer
        out2 = self.out2_conv(out2)

        # 2nd Upsampling
        out = self.up2(out)
      
        # 5th CONV block
        out = self.conv5(out)
        out = self.bn5(out)
        out = self.relu(out)
        out = self.avgpool(out)

        # # 6th CONV block
        out = self.conv6(out)
        out = self.bn6(out)
        out = self.relu(out)
        out = self.avgpool(out)

        # 2nd skip-connection
        out = out + out2
        out3 = out

        # 3rd Reducing Layer
        out3 = self.out3_conv(out3)


        # 3rd Upsampling
        out = self.up3(out)
        
        # 7th CONV block
        out = self.conv7(out)
        out = self.bn7(out)
        out = self.relu(out)
        out = self.avgpool(out)

        # 3rd skip-connection
        out = out + out3

        # 4th Upsampling
        out = self.up4(out)
        
       
        # Flatten
        out = torch.flatten(out, 1)

        # Last FC & Classification layers
        out = self.fc1(out)
        out = self.fc2(out)

        return out

"""### Training Function"""

def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()
    for batch, (X, y) in enumerate(dataloader):
      
        X, y = X.to(device), y.long().to(device)
        optimizer.zero_grad()
        torch.set_grad_enabled(True)
        
        pred = model(X).float()
       
        loss = loss_fn(pred, y)

       
        loss.backward()
        optimizer.step()

    
        if batch % 1000 == 0:
            loss, current = loss.item(), batch * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

"""### Testing Function"""

def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
          
            X, y = X.to(device), y.long().to(device)
            torch.set_grad_enabled(False)
            pred = model(X).float()
       
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
    return test_loss

"""### Defining "Learning Rate Scheduler" & "Early stopping mechanism""""

class LRScheduler():
  
    def __init__(
        self, optimizer, patience=5, min_lr=1e-7, factor=0.75
    ):
        
        self.optimizer = optimizer
        self.patience = patience
        self.min_lr = min_lr
        self.factor = factor
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( 
                self.optimizer,
                mode='min',
                patience=self.patience,
                factor=self.factor,
                min_lr=self.min_lr,
                verbose=True
            )
    def __call__(self, val_loss):
        self.scheduler.step(val_loss)


class EarlyStopping():
   
    def __init__(self, patience=10, min_delta=0):
      
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
    def __call__(self, val_loss):
        if self.best_loss == None:
            self.best_loss = val_loss
        elif self.best_loss - val_loss > self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        elif self.best_loss - val_loss < self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True

"""### Instantiating Dataset loaders for training & testing"""

normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])

trans = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            normalize,
        ])

trainset = TrainSet(f_list=file_list,id=id_list, transform=trans)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=5, shuffle=True, num_workers=4)

trans_test = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            normalize,
        ])

testset = TestTinyImageNetDataset(t_list=test_list,id=test_id, cls_id =id_list,  transform=trans_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False, num_workers=4)

"""### Instantiating & training & checkpoint "MyModel"

#### There are 2 cells for "MyModel" that are given below.
#### Run the 1st cell in case you want to load a checkpoint & resume training.
#### Else Run the 2nd cell in case you want to start training from scratch.

#### **NOTE: Make sure to change the path of loading the checkpoint in 1st cell**
"""

MyModel = MyModel()
MyModel = MyModel.to(device)
loss_fn = torch.nn.CrossEntropyLoss()
optimizer_MyModel = torch.optim.Adam(MyModel.parameters(), lr=0.0001)

# **NOTE: Make sure to change the path of loading the checkpoint in 1st cell**
checkpoint = torch.load('gdrive/MyDrive/trained-MyModel-model.ckpt')
MyModel.load_state_dict(checkpoint['model_state_dict'])
optimizer_MyModel.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch'] + 1
loss = checkpoint['loss']

es =  EarlyStopping()
lrs = LRScheduler(optimizer_MyModel)


checkpoint_dir = "."
test_loss = 0
epochs = 20

for i in range(epochs):
    print(f"Epoch {i+1}")
    train(trainloader, MyModel, loss_fn, optimizer_MyModel)
    test_loss = test(testloader, MyModel, loss_fn)

    torch.save({
        'epoch': i+1,
        'model_state_dict': MyModel.state_dict(),
        'optimizer_state_dict': optimizer_MyModel.state_dict(),
        'loss': test_loss
        }, checkpoint_dir+'/%04d-MyModel-model.ckpt' %i)
    
    lrs(test_loss)
    es(test_loss)
    if es.early_stop: 
         break

"""#### Run the 2nd cell in case you want to start training from scratch."""

MyModel = MyModel()
MyModel = MyModel.to(device)
loss_fn = torch.nn.CrossEntropyLoss()
optimizer_MyModel = torch.optim.Adam(MyModel.parameters(), lr=0.0001)


es =  EarlyStopping()
lrs = LRScheduler(optimizer_MyModel)

checkpoint_dir = "."
test_loss = 0
epochs = 20

for i in range(epochs):
    print(f"Epoch {i+1}")
    train(trainloader, MyModel, loss_fn, optimizer_MyModel)
    test_loss = test(testloader, MyModel, loss_fn)

    torch.save({
        'epoch': i+1,
        'model_state_dict': MyModel.state_dict(),
        'optimizer_state_dict': optimizer_MyModel.state_dict(),
        'loss': test_loss
        }, checkpoint_dir+'/%04d-MyModel-model.ckpt' %i)
    
    lrs(test_loss)
    es(test_loss)
    if es.early_stop: 
         break